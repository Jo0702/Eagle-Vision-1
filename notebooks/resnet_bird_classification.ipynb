{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #working with local files\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "from torch import nn, cuda, optim, device\n",
    "from torchvision import (models, #getting the pretrained resnet\n",
    "                         transforms, \n",
    "                         datasets) \n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list_of_dirs = [i.title() for i in os.listdir('train')]\n",
    "sorted_list_of_dirs.sort()\n",
    "\n",
    "idx_to_class_label = {\n",
    "    idx: class_label for idx, class_label in zip(range(len(sorted_list_of_dirs)), sorted_list_of_dirs)\n",
    "}\n",
    "\n",
    "with open('idx_to_class_label.json', 'w') as f:\n",
    "    json.dump(idx_to_class_label, f)\n",
    "    \n",
    "with open('idx_to_class_label.json', 'rb') as f:\n",
    "    j = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.require_grad = False\n",
    "    \n",
    "model.fc = nn.Sequential(nn.Linear(model.fc.in_features, 1024),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(0.30),\n",
    "                        nn.Linear(1024, 250))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if cuda.is_available() else 'cpu')\n",
    "\n",
    "model.to(device)\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder('train')\n",
    "val_data = datasets.ImageFolder('valid')\n",
    "test_data = datasets.ImageFolder('test')\n",
    "\n",
    "train_data.transform = train_transform\n",
    "val_data.transform = val_transform\n",
    "test_data.transform = test_transform\n",
    "\n",
    "batch_size=16\n",
    "\n",
    "train_loader = DataLoader(train_data,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "val_loader = DataLoader(val_data,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True)\n",
    "test_loader = DataLoader(test_data,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                      lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_loader, val_loader, model, criterion, optimizer, num_epochs=10, batch_size=16):\n",
    "    start = time.time()\n",
    "    best_model = model.state_dict()\n",
    "    best_acc = 0\n",
    "    train_loss_over_time = []\n",
    "    val_loss_over_time = []\n",
    "    train_acc_over_time = []\n",
    "    val_acc_over_time = []\n",
    "    early_stop_min_increase = 0.0001\n",
    "    early_stop_patience = 10\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch number: {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase=='train':\n",
    "                data_loader = train_loader\n",
    "                model.train()\n",
    "            else:\n",
    "                data_loader = val_loader\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for inputs, labels in data_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, pred = torch.max(outputs, dim=1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase=='train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(pred==labels.data)\n",
    "                        \n",
    "            if phase=='train':\n",
    "                epoch_loss = running_loss/len(train_loader.dataset)\n",
    "                train_loss_over_time.append(epoch_loss)\n",
    "                epoch_acc = running_corrects.double()/len(train_loader.dataset)\n",
    "                train_acc_over_time.append(epoch_acc)\n",
    "\n",
    "            else:\n",
    "                epoch_loss = running_loss/len(val_loader.dataset)\n",
    "                val_loss_over_time.append(epoch_loss)\n",
    "                epoch_acc = running_corrects.double()/len(val_loader.dataset)\n",
    "                val_acc_over_time.append(epoch_acc)\n",
    "                \n",
    "                #early stopping\n",
    "                #if the difference between this epoch valid acc and the previous epoch valid acc\n",
    "                #isn't greater than the desired min increase, then the model is not improving\n",
    "                #if the model is not improving for 5 epochs straight, early stop.\n",
    "#                 if len(val_acc_over_time) >= 5:\n",
    "# #                     prev_epoch_acc = val_acc_over_time[-2]\n",
    "#                     recent_epoch_accs = val_acc_over_time[-5:-1]\n",
    "#                     if epoch_acc - np.max(recent_epoch_accs) <= early_stop_min_increase:\n",
    "# #                     if (epoch_acc - prev_epoch_acc) <= early_stop_min_increase:\n",
    "#                         epochs_no_improve += 1\n",
    "#                         print('Model has not improved beyond threshold')\n",
    "#                     else:\n",
    "#                         if epochs_no_improve:\n",
    "#                             epochs_no_improve = 0\n",
    "#                             print('Model has improved, resetting early stop count')\n",
    "                    \n",
    "            print(f\"{phase} loss: {epoch_loss:.3f}, acc: {epoch_acc:.3f}\")\n",
    "                \n",
    "            if phase=='val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model = model.state_dict()\n",
    "                torch.save(model, 'trained_model_resnet50_checkpoint.pt') #checkpoint\n",
    "                epochs_no_improve = 0\n",
    "            elif phase == 'val' and epoch_acc < best_acc:\n",
    "                epochs_no_improve += 1\n",
    "                print(f\"Number of epochs without improvement has increased to {epochs_no_improve}\")\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            early_stop = True\n",
    "        if early_stop:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "        print('-'*60)\n",
    "    total_time = (time.time() - start)/60\n",
    "    print(f\"Training completed. Time taken: {total_time:.3f} min\\nBest accuracy: {best_acc:.3f}\")\n",
    "    model.load_state_dict(best_model)\n",
    "    loss = {'train': train_loss_over_time,\n",
    "           'val': val_loss_over_time}\n",
    "    acc = {'train': train_acc_over_time,\n",
    "          'val': val_acc_over_time}\n",
    "    return model, loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history, loss, acc = fit(train_loader, val_loader, model, criterion, optimizer,\n",
    "                        num_epochs=100, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader, model, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    preds = list()\n",
    "    labels_list = list()\n",
    "    \n",
    "    for inputs, labels in test_loader:\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, pred = torch.max(outputs, dim=1)\n",
    "            preds.append(pred)\n",
    "            labels_list.append(labels)\n",
    "            \n",
    "        test_loss += loss.item()*inputs.size(0)\n",
    "        correct = pred.eq(labels.data.view_as(pred))\n",
    "        accuracy = torch.mean(correct.type(torch.FloatTensor))\n",
    "        test_acc += accuracy.item() * inputs.size(0)\n",
    "        \n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "    test_acc = test_acc / len(test_loader.dataset)\n",
    "        \n",
    "    print(f\"Test loss: {test_loss:.4f}\\nTest acc: {test_acc:.4f}\")\n",
    "    \n",
    "    return preds, labels_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels = evaluate(val_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(np.array(torch.cat(labels)), np.array(torch.cat(predictions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(torch.cat(labels)), np.array(torch.cat(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = metrics.f1_score(np.array(torch.cat(labels)), np.array(torch.cat(predictions)), average='weighted')\n",
    "acc = metrics.accuracy_score(np.array(torch.cat(labels)), np.array(torch.cat(predictions)))\n",
    "recall = metrics.recall_score(np.array(torch.cat(labels)), np.array(torch.cat(predictions)), average='weighted')\n",
    "precision = metrics.precision_score(np.array(torch.cat(labels)), np.array(torch.cat(predictions)), average='weighted')\n",
    "\n",
    "print(f\"f1: {f1_score}\\nacc: {acc}\\nrecall: {recall}\\nprecision: {precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('trained_model_resnet50.pt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name):\n",
    "    torch.save(model, model_name)\n",
    "    \n",
    "def load_model(model_location):\n",
    "    return torch.load(model_location)\n",
    "\n",
    "save_model(model, 'trained_model_resnet50.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'state_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loded_model = load_model('trained_model_resnet50.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "l = os.listdir('consolidated/consolidated')\n",
    "l.sort()\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list_of_dirs = [i.title() for i in os.listdir('train')]\n",
    "sorted_list_of_dirs.sort()\n",
    "\n",
    "print(sorted_list_of_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_filepath, model, index_to_class_labels, show=True):\n",
    "    img = Image.open(image_filepath)\n",
    "    if show:\n",
    "        display(img)\n",
    "    img_t = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "    ])\n",
    "    img = img_t(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "    model.eval()\n",
    "    output_tensor = model(img)\n",
    "#     _, pred = torch.max(outputs, dim=1)\n",
    "    prob_tensor = torch.nn.Softmax(dim=1)(output_tensor)\n",
    "    top_5 = torch.topk(prob_tensor, 3, dim=1)\n",
    "    out = []\n",
    "    for pred_prob, pred_idx in zip(top_5.values.detach().numpy().flatten(), top_5.indices.detach().numpy().flatten()):\n",
    "        predicted_label = sorted_list_of_dirs[pred_idx]\n",
    "        predicted_prob = pred_prob*100\n",
    "        out.append((predicted_label, str(round(predicted_prob, 3))+'%'))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('test/BELTED KINGFISHER/1.jpg', model, sorted_list_of_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_prob, pred_idx in zip(t.values.detach().numpy().flatten(), t.indices.detach().numpy().flatten()):\n",
    "    predicted_label = sorted_list_of_dirs[pred_idx]\n",
    "    predicted_prob = pred_prob*100\n",
    "    print(f\"Predicted label: {predicted_label}; Probability: {predicted_prob:4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload model file to s3:\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file('trained_model_resnet50.pt', 'bird-classification-bucket', 'models/trained_model_resnet50.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
